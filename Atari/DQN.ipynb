{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from comet_ml import Experiment\n",
    "#experiment = Experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import argparse\n",
    "from collections import namedtuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.spaces import Box\n",
    "\n",
    "import cv2\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoopEnv(gym.Wrapper):\n",
    "    def __init__(self, env, noop_max=30):\n",
    "        super().__init__(env)\n",
    "        self.noop_max = noop_max\n",
    "        self.override_num_noops = None\n",
    "        self.noop_action = 0\n",
    "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
    "        \n",
    "    def reset(self, **kwargs):\n",
    "        self.env.reset(**kwargs)\n",
    "        if self.override_num_noops is not None:\n",
    "            noops = self.override_num_noops\n",
    "        else:\n",
    "            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1)\n",
    "        assert noops > 0\n",
    "        obs = None\n",
    "        for _ in range(noops):\n",
    "            obs, _, done, _ = self.env.step(self.noop_action)\n",
    "            if done:\n",
    "                obs = self.env.reset(**kwargs)\n",
    "        return obs\n",
    "    \n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpisodicLifeEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.lives = 0\n",
    "        self.was_real_done = True\n",
    "        \n",
    "    def step(self, action):\n",
    "        obs, reward,  done, info = self.env.step(action)\n",
    "        self.was_real_done = done\n",
    "        lives = self.env.unwrapped.ale.lives()\n",
    "        if lives < self.lives and lives > 0:\n",
    "            done = True\n",
    "        self.lives = lives\n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        if self.was_real_done:\n",
    "            obs = self.env.reset(**kwargs)\n",
    "        else:\n",
    "            obs, _, _, _ = self.env.step(0)\n",
    "        self.lives = self.env.unwrapped.ale.lives()\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        super().__init__(env)\n",
    "        self.obs_buffer = np.zeros((2,) + env.observation_space.shape, dtype=np.uint8)\n",
    "        self.skip = skip\n",
    "    \n",
    "    def step(self, action):\n",
    "        total_reward = 0.\n",
    "        done = None\n",
    "        for i in range(self.skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            if i == self.skip - 2:\n",
    "                self.obs_buffer[0] = obs\n",
    "            if i == self.skip - 1:\n",
    "                self.obs_buffer[1] = obs\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        max_frame = self.obs_buffer.max(axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        super().__init__(env)\n",
    "        self.skip = skip\n",
    "    \n",
    "    def step(self, action):\n",
    "        total_reward = 0.\n",
    "        done = None\n",
    "        for i in range(self.skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IncScoreEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        reward += 0.1\n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarpFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.width = 128\n",
    "        self.height = 128\n",
    "        self.observation_space = Box(low=0, high=255, shape=(self.height, self.width, 1), dtype=np.uint8)\n",
    "        \n",
    "    def observation(self, frame):\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
    "        return frame[:, :, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapForPyTorch(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape # H, W, C -> C, H, W\n",
    "        self.observation_space = Box(self.observation_space.low[0,0,0], self.observation_space.high[0,0,0],\n",
    "                                    [obs_shape[2], obs_shape[0], obs_shape[1]], dtype=self.observation_space.dtype)\n",
    "        \n",
    "    def observation(self, observation):\n",
    "        return observation.transpose(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Util:\n",
    "    @staticmethod\n",
    "    def make_env(env_id, seed=None):\n",
    "        def _thunk():\n",
    "            env = gym.make(env_id)\n",
    "            env = NoopEnv(env, noop_max=30)\n",
    "            env = MaxAndSkipEnv(env, skip=4)\n",
    "            if seed is not None:\n",
    "                env.seed(seed)\n",
    "            env = EpisodicLifeEnv(env)\n",
    "            #env = IncScoreEnv(env)\n",
    "            env = WarpFrame(env)\n",
    "            env = WrapForPyTorch(env)\n",
    "            return env\n",
    "        return _thunk\n",
    "\n",
    "    @staticmethod\n",
    "    def make_env_play(env_id, seed=None):\n",
    "        def _thunk():\n",
    "            env = gym.make(env_id)\n",
    "            env = SkipEnv(env, skip=4)\n",
    "            if seed is not None:\n",
    "                env.seed(seed)\n",
    "            return env\n",
    "        return _thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(in_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = self.shortcut(x)\n",
    "        return F.relu(self.residual(x) + shortcut, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_nc):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Pointwise Convolution\n",
    "        self.query_conv = nn.Conv2d(input_nc, input_nc // 8, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(input_nc, input_nc // 8, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(input_nc, input_nc, kernel_size=1)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=-2)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        proj_query = self.query_conv(x).view(x.shape[0], -1, x.shape[2] * x.shape[3]).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x).view(x.shape[0], -1, x.shape[2] * x.shape[3])\n",
    "        s = torch.bmm(proj_query, proj_key) # バッチ毎の行列乗算\n",
    "        attention_map_T = self.softmax(s)\n",
    "        \n",
    "        proj_value = self.value_conv(x).view(x.shape[0], -1, x.shape[2] * x.shape[3])\n",
    "        o = torch.bmm(proj_value, attention_map_T)\n",
    "        \n",
    "        o = o.view(x.shape[0], x.shape[1], x.shape[2], x.shape[3])\n",
    "        out = x + self.gamma * o\n",
    "        \n",
    "        return out#, attention_map_T.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, n_out, num_channels=1, conv_dim=32, n_repeat=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        model = [\n",
    "            nn.Conv2d(num_channels, conv_dim, kernel_size=3, stride=2, padding=1), #128x128 -> 64x64\n",
    "            nn.InstanceNorm2d(conv_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "\n",
    "        for _ in range(n_repeat): # -> 64x64 /= 2 ** n -> 8x8\n",
    "            model += [\n",
    "                nn.Conv2d(conv_dim, conv_dim * 2, kernel_size=3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(conv_dim),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            conv_dim *= 2\n",
    "            \n",
    "        self.net_base = nn.Sequential(*model)\n",
    "        self.self_attention = SelfAttention(conv_dim)\n",
    "        \n",
    "        self.conv_out = nn.Sequential(\n",
    "            nn.Conv2d(conv_dim, n_out, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(conv_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x /= 255.0\n",
    "        x = self.net_base(x)\n",
    "        x = self.self_attention(x)\n",
    "        x = self.conv_out(x)\n",
    "        x = F.adaptive_avg_pool2d(x, 1) # Global Average Pooling\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        return x\n",
    "    \n",
    "    def act(self, x):\n",
    "        output = self(x)\n",
    "        probs = F.softmax(output, dim=1)\n",
    "        action = probs.multinomial(num_samples=1)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.index = 0\n",
    "        \n",
    "    def push(self, state, action, next_state, reward):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.index] = Transition(state, action, next_state, reward)\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Brain:\n",
    "    def __init__(self, use_cpu, lr, gamma, batch_size, mem_capacity, num_actions):\n",
    "        use_cuda = torch.cuda.is_available() if not use_cpu else False\n",
    "        self.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "        print(f'Use Device: {self.device}')\n",
    "\n",
    "        self.net = Net(num_actions).to(self.device)\n",
    "        self.net.apply(self.weights_init)\n",
    "        self.memory = ReplayMemory(mem_capacity)\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        filename = 'weight.pth'\n",
    "        if os.path.exists(filename):\n",
    "            param = torch.load(filename, map_location=self.device)\n",
    "            self.net.load_state_dict(param)\n",
    "            print(f'loaded: {filename}')\n",
    "        \n",
    "    def weights_init(self, module):\n",
    "        if type(module) == nn.Conv2d or type(module) == nn.Linear:\n",
    "            nn.init.kaiming_normal_(module.weight)\n",
    "            module.bias.data.fill_(0)\n",
    "        \n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        self.net.train()\n",
    "        \n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        state_batch = torch.cat(batch.state).to(self.device)\n",
    "        action_batch = torch.cat(batch.action).to(self.device)\n",
    "        reward_batch = torch.cat(batch.reward).to(self.device)\n",
    "        \n",
    "        # 現在の状態\n",
    "        state_action_values = self.net(state_batch).gather(1, action_batch).squeeze(0)\n",
    "        \n",
    "        # 次の状態\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None]).to(self.device)\n",
    "        non_final_mask = torch.ByteTensor(tuple(map(lambda s: s is not None, batch.next_state))).to(self.device)\n",
    "        next_state_values = torch.zeros(self.batch_size).to(self.device)\n",
    "        next_state_values[non_final_mask] = self.net(non_final_next_states).max(1)[0].detach()\n",
    "        \n",
    "        # 報酬から次のQ値を推定\n",
    "        expected_state_action_values = reward_batch + self.gamma * next_state_values\n",
    "        \n",
    "        # Q値の誤差のL1ノルム\n",
    "        loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "        \n",
    "        # 誤差逆伝搬によるNNの勾配降下更新\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def decide_action(self, state):\n",
    "        self.net.eval()\n",
    "        return self.net.act(state.to(self.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, use_cpu, num_actions, lr, gamma, batch_size, mem_capacity):\n",
    "        self.brain = Brain(use_cpu, lr, gamma, batch_size, mem_capacity, num_actions)\n",
    "        \n",
    "    def update_q_functions(self):\n",
    "        return self.brain.replay()\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        return self.brain.decide_action(state)\n",
    "\n",
    "    def memorize(self, state, action, next_state, reward):\n",
    "        self.brain.memory.push(state, action, next_state, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, env_name, use_cpu, lr, gamma, batch_size, mem_capacity, num_updates):\n",
    "        seed = None\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            if not use_cpu:\n",
    "                torch.cuda.manual_seed(seed)\n",
    "        \n",
    "        self.env = Util.make_env(env_name, seed)()\n",
    "        self.env_play = Util.make_env_play(env_name, seed)()\n",
    "        \n",
    "        self.num_actions = self.env.action_space.n\n",
    "        self.agent = Agent(use_cpu, self.num_actions, lr, gamma, batch_size, mem_capacity)\n",
    "        \n",
    "        self.num_updates = num_updates\n",
    "        \n",
    "    def train(self, weight_dir):\n",
    "        state = self.env.reset()\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        rewards = []\n",
    "        frame_num = 0\n",
    "        max_frame_num = 0\n",
    "        done_num = 0\n",
    "        for i in tqdm(range(self.num_updates)):\n",
    "            action = self.agent.get_action(state)\n",
    "            state_next, reward, done, _ = self.env.step(action)\n",
    "            frame_num += 1\n",
    "            \n",
    "            rewards.append(reward)\n",
    "            \n",
    "            state_next = torch.from_numpy(state_next).float().unsqueeze(0)\n",
    "            reward = torch.FloatTensor([reward])\n",
    "            self.agent.memorize(state, action, state_next, reward)\n",
    "            loss = self.agent.update_q_functions()\n",
    "            state = state_next\n",
    "            \n",
    "            loss = loss.cpu().item() if loss is not None else None\n",
    "            #experiment.log_metric('Loss', loss)\n",
    "            \n",
    "            self.env.render()\n",
    "            #print(f'action: {action.cpu().item()} reward: {reward.item()}')\n",
    "            \n",
    "            if done:\n",
    "                state = self.env.reset()\n",
    "                state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "                done_num += 1\n",
    "                max_frame_num = max(max_frame_num, frame_num)\n",
    "                frame_num = 0\n",
    "                \n",
    "            if (i+1) % 100 == 0:\n",
    "                print(f'finished frames: {i+1}, max frames: {max_frame_num}, finished: {done_num}, sum rewards: {sum(rewards):.1f}, loss: {loss}')\n",
    "                rewards = []\n",
    "                done_num = 0\n",
    "                max_frame_num = 0\n",
    "                \n",
    "            if (i+1) % 10000 == 0:\n",
    "                torch.save(self.agent.brain.net.state_dict(), os.path.join(weight_dir, f'weight.{i+1}.pth'))\n",
    "        torch.save(self.agent.brain.net.state_dict(), os.path.join(weight_dir, 'weight.latest.pth'))\n",
    "    \n",
    "    def save_movie(self):\n",
    "        state = self.env.reset()\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        frames = [self.env_play.reset()]\n",
    "        frame_num = 0\n",
    "        while True:\n",
    "            action = self.agent.get_action(state)\n",
    "            state_next, reward, done, _ = self.env.step(action)\n",
    "            \n",
    "            frame, _, _, _ = self.env_play.step(action)\n",
    "            frames.append(frame)\n",
    "            frame_num += 1\n",
    "            \n",
    "            state_next = torch.from_numpy(state_next).float().unsqueeze(0)\n",
    "            reward = torch.FloatTensor([reward])\n",
    "            self.agent.memorize(state, action, state_next, reward)\n",
    "            state = state_next\n",
    "            \n",
    "            self.env.render()\n",
    "            self.env_play.render()\n",
    "            \n",
    "            if done:\n",
    "                #state = self.env.reset()\n",
    "                #state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "                #frames = [self.env_play.reset()]\n",
    "                #frame_num = 0\n",
    "                break\n",
    "                \n",
    "        display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_frames_as_gif(frames):\n",
    "    %matplotlib inline\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib import animation\n",
    "    from IPython.display import HTML\n",
    "\n",
    "    base = min(frames[0].shape[1], frames[0].shape[0])\n",
    "    plt.figure(figsize=(frames[0].shape[1] / base * 6, frames[0].shape[0] / base * 6), dpi=72, tight_layout=True)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    \n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "        \n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=20)\n",
    "    anim.save('output.gif', writer=animation.PillowWriter())\n",
    "    HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    hyper_params = {}\n",
    "    hyper_params['Env Name'] = args.env_name\n",
    "    hyper_params['Wieght Dir'] = args.weight_dir\n",
    "    hyper_params['Learning Rate'] = args.lr\n",
    "    hyper_params['Gamma(Q-Learning)'] = args.gamma\n",
    "    hyper_params['Batch Size'] = args.batch_size\n",
    "    hyper_params['Memory Capacity'] = args.mem_capacity\n",
    "    hyper_params['Num Updates'] = args.num_updates\n",
    "    \n",
    "    for key in hyper_params.keys():\n",
    "        print(f'{key}: {hyper_params[key]}')\n",
    "    #experiment.log_parameters(hyper_params)\n",
    "    \n",
    "    env = Environment(args.env_name, args.cpu, args.lr, args.gamma, args.batch_size, args.mem_capacity, args.num_updates)\n",
    "    env.train(args.weight_dir)\n",
    "    #env.save_movie()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--env_name', type=str, default='BreakoutNoFrameskip-v4')\n",
    "    #parser.add_argument('--env_name', type=str, default='BeamRider-v0')\n",
    "    parser.add_argument('--weight_dir', type=str, default='weights')\n",
    "    parser.add_argument('--lr', type=float, default=1e-10)\n",
    "    parser.add_argument('--gamma', type=float, default=0.99)\n",
    "    parser.add_argument('--batch_size', type=int, default=32)\n",
    "    parser.add_argument('--mem_capacity', type=int, default=10000)\n",
    "    parser.add_argument('--num_updates', type=int, default=int(1e5))\n",
    "    parser.add_argument('--cpu', action='store_true')\n",
    "    \n",
    "    args, unknown = parser.parse_known_args()\n",
    "    \n",
    "    if not os.path.exists(args.weight_dir):\n",
    "        os.mkdir(args.weight_dir)\n",
    "    \n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
