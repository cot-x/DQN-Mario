{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from comet_ml import Experiment\n",
    "#experiment = Experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import namedtuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.spaces import Box\n",
    "\n",
    "import ppaquette_gym_super_mario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoopEnv(gym.Wrapper):\n",
    "    def __init__(self, env, noop_max=30):\n",
    "        super().__init__(env)\n",
    "        self.noop_max = noop_max\n",
    "        self.override_num_noops = None\n",
    "        num_actions = env.action_space.shape[0]\n",
    "        self.noop_action = np.zeros(num_actions)\n",
    "        \n",
    "    def reset(self, **kwargs):\n",
    "        self.env.reset(**kwargs)\n",
    "        if self.override_num_noops is not None:\n",
    "            noops = self.override_num_noops\n",
    "        else:\n",
    "            noops = np.random.randint(1, self.noop_max + 1) # [1, noop_max+1)\n",
    "        obs = None\n",
    "        for _ in range(noops):\n",
    "            obs, _, done, _ = self.env.step(self.noop_action)\n",
    "            if done:\n",
    "                obs = self.env.reset(**kwargs)\n",
    "        return obs\n",
    "    \n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        super().__init__(env)\n",
    "        self.obs_buffer = np.zeros((2,) + env.observation_space.shape, dtype=np.uint8)\n",
    "        self.skip = skip\n",
    "    \n",
    "    def step(self, action):\n",
    "        total_reward = 0.\n",
    "        done = None\n",
    "        for i in range(self.skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            if i == self.skip - 2:\n",
    "                self.obs_buffer[0] = obs\n",
    "            if i == self.skip - 1:\n",
    "                self.obs_buffer[1] = obs\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        max_frame = self.obs_buffer.max(axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        super().__init__(env)\n",
    "        self.skip = skip\n",
    "    \n",
    "    def step(self, action):\n",
    "        total_reward = 0.\n",
    "        done = None\n",
    "        for i in range(self.skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformsFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.width = 128\n",
    "        self.height = 128\n",
    "        self.observation_space = Box(low=0, high=1, shape=(self.height, self.width, 1), dtype=np.float64)\n",
    "        \n",
    "    def observation(self, frame):\n",
    "        frame = frame.transpose(2, 0, 1) # H, W, C -> C, H, W\n",
    "        frame = torch.from_numpy(frame).float().unsqueeze(0) / 255.0\n",
    "        frame_tarns = transforms.functional.resize(frame, (self.width, self.height))\n",
    "        frame_tarns = transforms.functional.rgb_to_grayscale(frame_tarns)\n",
    "        return (frame_tarns, frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_nc):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Pointwise Convolution\n",
    "        self.query_conv = nn.Conv2d(input_nc, input_nc // 8, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(input_nc, input_nc // 8, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(input_nc, input_nc, kernel_size=1)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=-2)\n",
    "        self.gamma = nn.Parameter(torch.randn(1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        proj_query = self.query_conv(x).view(x.shape[0], -1, x.shape[2] * x.shape[3]).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x).view(x.shape[0], -1, x.shape[2] * x.shape[3])\n",
    "        s = torch.bmm(proj_query, proj_key) # バッチ毎の行列乗算\n",
    "        attention_map_T = self.softmax(s)\n",
    "        \n",
    "        proj_value = self.value_conv(x).view(x.shape[0], -1, x.shape[2] * x.shape[3])\n",
    "        o = torch.bmm(proj_value, attention_map_T)\n",
    "        \n",
    "        o = o.view(x.shape[0], x.shape[1], x.shape[2], x.shape[3])\n",
    "        out = x + self.gamma * o\n",
    "        \n",
    "        return out#, attention_map_T.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out=None, dim_depth=16, heads=4, dim_u=1, dim_recept=23):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        \n",
    "        dim_out = dim_in if dim_out is None else dim_out\n",
    "        \n",
    "        self.dim_depth = dim_depth\n",
    "        self.dim_u = dim_u\n",
    "        assert (dim_out % heads) == 0, 'must divide by heads for multi-head query'\n",
    "        dim_v = dim_out // heads\n",
    "        self.dim_v = dim_v\n",
    "        \n",
    "        self.queries = nn.Sequential(\n",
    "            nn.Conv2d(dim_in, dim_depth * heads, 1, bias=False),\n",
    "            nn.BatchNorm2d(dim_depth * heads)\n",
    "        )\n",
    "        self.keys = nn.Conv2d(dim_in, dim_depth * dim_u, 1, bias=False)\n",
    "        self.values = nn.Sequential(\n",
    "            nn.Conv2d(dim_in, dim_v * dim_u, 1, bias=False),\n",
    "            nn.BatchNorm2d(dim_v * dim_u)\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        self.local_context = True if dim_recept > 0 else False\n",
    "        if self.local_context:\n",
    "            assert (dim_recept % 2) == 1, 'receptive kernel size must be odd'\n",
    "            r = dim_recept\n",
    "            self.embedding = nn.Parameter(torch.randn([dim_depth, dim_u, 1, r, r]))\n",
    "            self.padding = (r - 1) // 2\n",
    "        else:\n",
    "            self.embedding = nn.Parameter(torch.randn([dim_depth, dim_u]))\n",
    "        \n",
    "        self.gamma = nn.Parameter(torch.randn(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "\n",
    "        queries = self.queries(x).view(b, self.heads, self.dim_depth, h * w)\n",
    "        softmax = self.softmax(self.keys(x).view(b, self.dim_depth, self.dim_u, h * w))\n",
    "        values = self.values(x).view(b, self.dim_v, self.dim_u, h * w)\n",
    "\n",
    "        lambda_c = torch.einsum('bkun,bvun->bkv', softmax, values)\n",
    "        y_c = torch.einsum('bhkn,bkv->bhvn', queries, lambda_c)\n",
    "\n",
    "        if self.local_context:\n",
    "            values = values.view(b, self.dim_u, -1, h, w)\n",
    "            lambda_p = F.conv3d(values, self.embedding, padding=(0, self.padding, self.padding))\n",
    "            lambda_p = lambda_p.view(b, self.dim_depth, self.dim_v, h * w)\n",
    "            y_p = torch.einsum('bhkn,bkvn->bhvn', queries, lambda_p)\n",
    "        else:\n",
    "            lambda_p = torch.einsum('ku,bvun->bkvn', self.embedding, values)\n",
    "            y_p = torch.einsum('bhkn,bkvn->bhvn', queries, lambda_p)\n",
    "\n",
    "        out = y_c + y_p\n",
    "        out = out.reshape(b, -1, h, w)\n",
    "        \n",
    "        out = out + self.gamma * out\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, n_out, num_channels=1, conv_dim=64, n_repeat=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        model = [\n",
    "            nn.Conv2d(num_channels, conv_dim, kernel_size=3, stride=2, padding=1), #128x128 -> 64x64\n",
    "            nn.InstanceNorm2d(conv_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "\n",
    "        for _ in range(n_repeat): # -> 64x64 /= 2 ** n -> 8x8\n",
    "            model += [\n",
    "                nn.Conv2d(conv_dim, conv_dim * 2, kernel_size=3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(conv_dim),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            conv_dim *= 2\n",
    "            \n",
    "        self.net_base = nn.Sequential(*model)\n",
    "        #self.self_attention = SelfAttention(conv_dim)\n",
    "        self.self_attention = LambdaLayer(conv_dim)\n",
    "        \n",
    "        self.conv_out = nn.Sequential(\n",
    "            nn.Conv2d(conv_dim, n_out, kernel_size=3, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(conv_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.net_base(x)\n",
    "        x = self.self_attention(x)\n",
    "        x = self.conv_out(x)\n",
    "        x = F.adaptive_max_pool2d(x, 1) # Global Average Pooling\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        return x\n",
    "    \n",
    "    def act(self, x):\n",
    "        output = self(x)\n",
    "        probs = F.softmax(output, dim=1)\n",
    "        action = probs.multinomial(num_samples=1)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.index = 0\n",
    "        \n",
    "    def push(self, state, action, next_state, reward):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.index] = Transition(state, action, next_state, reward)\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Brain:\n",
    "    def __init__(self, use_cpu, lr, gamma, batch_size, mem_capacity, num_actions):\n",
    "        use_cuda = torch.cuda.is_available() if not use_cpu else False\n",
    "        self.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "        print(f'Use Device: {self.device}')\n",
    "\n",
    "        self.net = Net(num_actions).to(self.device)\n",
    "        self.net.apply(self.weights_init)\n",
    "        self.memory = ReplayMemory(mem_capacity)\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.load_state()\n",
    "        \n",
    "    def weights_init(self, module):\n",
    "        if type(module) == nn.Linear or type(module) == nn.Conv2d or type(module) == nn.ConvTranspose2d:\n",
    "            nn.init.kaiming_normal_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.fill_(0)\n",
    "    \n",
    "    def save_state(self, weight_dir, epoch):\n",
    "        self.net.cpu()\n",
    "        torch.save(self.net.state_dict(), os.path.join(weight_dir, f'weight.{epoch}.pth'))\n",
    "        self.net.to(self.device)\n",
    "    \n",
    "    def load_state(self):\n",
    "        if os.path.exists('weight.pth'):\n",
    "            self.net.load_state_dict(torch.load('weight.pth', map_location=self.device))\n",
    "            print('Loaded network state.')\n",
    "        \n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        self.net.train()\n",
    "        \n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        state_batch = torch.cat(batch.state).to(self.device)\n",
    "        action_batch = torch.cat(batch.action).to(self.device)\n",
    "        reward_batch = torch.cat(batch.reward).to(self.device)\n",
    "        \n",
    "        # 現在の状態\n",
    "        state_action_values = self.net(state_batch).gather(1, action_batch)\n",
    "        \n",
    "        # 次の状態\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None]).to(self.device)\n",
    "        non_final_mask = torch.BoolTensor(tuple(map(lambda s: s is not None, batch.next_state))).to(self.device)\n",
    "        next_state_values = torch.zeros(self.batch_size).to(self.device)\n",
    "        next_state_values[non_final_mask] = self.net(non_final_next_states).max(dim=1)[0].detach()\n",
    "        \n",
    "        # 報酬から次のQ値を推定\n",
    "        expected_state_action_values = reward_batch + self.gamma * next_state_values\n",
    "        expected_state_action_values = expected_state_action_values.unsqueeze(1).repeat(1, state_action_values.size(1))\n",
    "        \n",
    "        # Q値の誤差のL1ノルム\n",
    "        loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "        \n",
    "        # 誤差逆伝搬によるNNの勾配降下更新\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def array_action(self, x, num_actions):\n",
    "        num_buttons = int(math.log2(num_actions))\n",
    "        b = format(x, f'0{num_buttons}b') # 2進数\n",
    "        return np.array([int(b[i]) for i in range(num_buttons)])\n",
    "    \n",
    "    def decide_action(self, state):\n",
    "        self.net.eval()\n",
    "        action = self.net.act(state.to(self.device))[0][0].cpu().item()\n",
    "        return self.array_action(action, self.num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, use_cpu, num_actions, lr, gamma, batch_size, mem_capacity):\n",
    "        self.brain = Brain(use_cpu, lr, gamma, batch_size, mem_capacity, num_actions)\n",
    "        \n",
    "    def update_q_functions(self):\n",
    "        return self.brain.replay()\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        return self.brain.decide_action(state)\n",
    "\n",
    "    def memorize(self, state, action, next_state, reward):\n",
    "        self.brain.memory.push(state, action, next_state, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        \n",
    "        self.env = self.make_env(self.args.env_name)\n",
    "        self.env_play = None\n",
    "        \n",
    "        self.num_buttons = self.env.action_space.shape[0]\n",
    "        self.num_actions = 2 ** self.num_buttons\n",
    "        self.agent = Agent(args.cpu, self.num_actions, args.lr, args.gamma, args.batch_size, args.mem_capacity)\n",
    "        \n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "        if self.env_play:\n",
    "            self.env_play.close()\n",
    "    \n",
    "    def make_env(self, env_name):\n",
    "        env = gym.make(env_name)\n",
    "        env = NoopEnv(env, noop_max=30)\n",
    "        env = MaxAndSkipEnv(env, skip=4)\n",
    "        env = TransformsFrame(env)\n",
    "        return env\n",
    "    \n",
    "    def make_env_play(self, env_name):\n",
    "        env = gym.make(env_name)\n",
    "        env = SkipEnv(env, skip=4)\n",
    "        env = TransformsFrame(env)\n",
    "        return env\n",
    "    \n",
    "    def train(self):\n",
    "        hyper_params = {}\n",
    "        hyper_params['Env Name'] = self.args.env_name\n",
    "        hyper_params['Wieght Dir'] = self.args.weight_dir\n",
    "        hyper_params['Learning Rate'] = self.args.lr\n",
    "        hyper_params['Gamma(Q-Learning)'] = self.args.gamma\n",
    "        hyper_params['Batch Size'] = self.args.batch_size\n",
    "        hyper_params['Memory Capacity'] = self.args.mem_capacity\n",
    "        hyper_params['Num Updates'] = self.args.num_updates\n",
    "        \n",
    "        for key in hyper_params.keys():\n",
    "            print(f'{key}: {hyper_params[key]}')\n",
    "        #experiment.log_parameters(hyper_params)\n",
    "        \n",
    "        (state, _) = self.env.reset()\n",
    "        done_num = 0\n",
    "        episode_reward= 0\n",
    "        rewards = []\n",
    "        for i in tqdm(range(self.args.num_updates)):\n",
    "            action = self.agent.get_action(state)\n",
    "            (state_next, _), reward, done, _ = self.env.step(action)\n",
    "            #print(f'action: {action} reward: {reward}')\n",
    "            \n",
    "            episode_reward += reward\n",
    "            \n",
    "            reward = torch.FloatTensor([reward])\n",
    "            action = torch.from_numpy(action).unsqueeze(0)\n",
    "            self.agent.memorize(state, action, state_next, reward)\n",
    "            loss = self.agent.update_q_functions()\n",
    "            state = state_next\n",
    "            \n",
    "            loss = loss.cpu().item() if loss is not None else None\n",
    "            #experiment.log_metric('Loss', loss)\n",
    "            \n",
    "            if done:\n",
    "                (state, _) = self.env.reset()\n",
    "                rewards += [episode_reward]\n",
    "                episode_reward = 0\n",
    "                done_num += 1\n",
    "                print(f'finished frames {i+1}, {done_num} times finished, reward {rewards[-1]:.1f}')\n",
    "            \n",
    "            if (i+1) % 1000 == 0:\n",
    "                self.agent.brain.save_state(self.args.weight_dir, i+1)\n",
    "    \n",
    "    def save_movie(self):\n",
    "        self.env_play = self.make_env_play(self.args.env_name)\n",
    "        (state, frame) = self.env_play.reset()\n",
    "        frames = [state[0]]\n",
    "        \n",
    "        while True:\n",
    "            action = self.agent.get_action(state)\n",
    "            (state, frame), _, done, _ = self.env_play.step(action)\n",
    "            frames += [frame[0]]\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        self.env_play.close()\n",
    "        self.save_frames_as_gif(frames)\n",
    "        \n",
    "    def save_frames_as_gif(self, frames):\n",
    "        import matplotlib.pyplot as plt\n",
    "        from matplotlib import animation\n",
    "        \n",
    "        ToPIL = transforms.ToPILImage()\n",
    "        \n",
    "        base = min(frames[0].shape[1], frames[0].shape[2])\n",
    "        plt.figure(figsize=(frames[0].shape[1] / base * 6, frames[0].shape[2] / base * 6), dpi=72, tight_layout=True)\n",
    "        patch = plt.imshow(ToPIL(frames[0]))\n",
    "        plt.axis('off')\n",
    "\n",
    "        def animate(i):\n",
    "            patch.set_data(ToPIL(frames[i]))\n",
    "\n",
    "        anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=1)\n",
    "        anim.save('output.gif', writer=animation.PillowWriter(fps=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    env = Environment(args)\n",
    "    try:\n",
    "        if args.savemovie:\n",
    "            env.save_movie()\n",
    "            return\n",
    "\n",
    "        env.train()\n",
    "    finally:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--env_name', type=str, default='ppaquette/SuperMarioBros-1-1-v0')\n",
    "    parser.add_argument('--weight_dir', type=str, default='weights')\n",
    "    parser.add_argument('--lr', type=float, default=0.0001)\n",
    "    parser.add_argument('--gamma', type=float, default=0.99)\n",
    "    parser.add_argument('--batch_size', type=int, default=32)\n",
    "    parser.add_argument('--mem_capacity', type=int, default=10000)\n",
    "    parser.add_argument('--num_updates', type=int, default=100000)\n",
    "    parser.add_argument('--savemovie', action='store_true')\n",
    "    parser.add_argument('--cpu', action='store_true')\n",
    "    \n",
    "    args, unknown = parser.parse_known_args()\n",
    "    \n",
    "    if not os.path.exists(args.weight_dir):\n",
    "        os.mkdir(args.weight_dir)\n",
    "    \n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
